








# This is a single-line comment

"""
This is a
multiline
comment
"""














# the following code has a hierarchy, whereby `print(c)` is a child of `for c in "example"`
for c in "example":
    print(c)








w = 1
x = -3255522
y = 1.09834
z = -20.976362

print(type(w))
print(type(x))
print(type(y))
print(type(z))





text = "This is a sample sentence."

for c in text:
    print(c)





for c in text.split():
    print(c)





ingredients = ["chicken", "curry spices", "yoghurt", "coconut milk"]





for i in ingredients:
    print(i)





# Lists are ordered and indexed

print(ingredients[1])


# Lists are changeable
ingredients[2] = "vegan yoghurt"
print(ingredients)


# Lists may contain duplicate values
ingredients.append("chicken")
# we need A LOT of chicken!
print(ingredients)


# How many items are in a list?
print(len(ingredients))





shopping_list = {
    "chicken": "1 whole",
    "curry spices": "150gr",
    "yoghurt": "200gr",
    "coconut milk": "400ml",
}

print(shopping_list)


# Dictionaries are ordered
print(shopping_list["chicken"])


# Dictionaries are changeable
shopping_list["yoghurt"] = "400gr"
print(shopping_list)


# Dictionaries do not allow duplicates
shopping_list = {
    "chicken": "1 whole",
    "curry spices": "150gr",
    "yoghurt": "200gr",
    "coconut milk": "400ml",
    "coconut milk": "300ml",
}

print(shopping_list)


# How many items are in a dictionary?
print(len(shopping_list))











with open("./data/instagram/2022-01-02_14-00-14_UTC.txt", "r") as text:
    print(text)
    


with open("./data/instagram/2022-01-02_14-00-14_UTC.txt", "r") as text:
    print(text.read())





from glob import glob

files = glob("./data/instagram/*.txt")

for file in files:
    text = open(file, "r").read()
    print(text)





ingredients = ["chicken", "curry spices", "yoghurt", "coconut milk"]
more_ingredients = ["rice","ghee","salt","bay leaves"]


out = open("list_of_ingredients.txt", "w")
for i in ingredients:
    out.write(f"{i}\n")
out.close()


out = open("list_of_ingredients.txt", "w")
for m in more_ingredients:
    out.write(f"{m}\n")
out.close()


out = open("list_of_ingredients.txt", "a")
for i in ingredients:
    out.write(f"{i}\n")
out.close()


with open("list_of_ingredients.txt", "a") as out:
    for i in ingredients:
        out.write(f"{i}\n")
    for m in more_ingredients:
        out.write(f"{m}\n")
        














from lingua import Language, LanguageDetectorBuilder
from glob import glob
import re

# Setup some variables and parameters to be later used
languages = [Language.ENGLISH, Language.FRENCH]
detector = LanguageDetectorBuilder.from_languages(*languages).build()


posts = glob("./data/instagram/*.txt")

for post in posts:
    f = open(post, "r").readlines()
    for line in f:
        lang = detector.detect_language_of(line)
        print(f"TEXT::{line}\nLANG::{lang}\n")
        





posts = glob("./data/instagram/*.txt")

for post in posts:
    f = open(post, "r").readlines()
    post_lines = {}
    recognised_lang = ""
    post_name = re.sub(".*?instagram\/", "", post).replace("/", "").replace(".txt", "")
    for line in f:
        lang = detector.detect_language_of(line)
        if lang is None:
            lang = recognised_lang
        else:
            post_lines[f"{line}"] = lang
    lines_en = [k for k,v in post_lines.items() if str(v) == "Language.ENGLISH"]
    lines_fr = [k for k,v in post_lines.items() if str(v) == "Language.FRENCH"]
    post_en = open(f"{post_name}_EN.txt", "a").write("\n".join(lines_en))
    post_fr = open(f"{post_name}_FR.txt", "a").write(lines_fr)





import emoji

# Define a custom function to transliterate emojis and add curly brackets as delimites for the CLDR

def demoji(chars, data_dict):
    trans = emoji.demojize(chars, delimiters=("{", "}"))
    return trans


posts = glob("./data/instagram/*.txt")

for post in posts:
    f = open(post, "r").readlines()
    for line in f:
        line = emoji.replace_emoji(line, replace=lambda chars, data_dict: demoji(chars,data_dict))
        print(line)





import re
import wordsegment

wordsegment.load()
hashtag_re = re.compile("(?:^|\s)([ï¼ƒ#]{1})(\w+)", re.UNICODE)


posts = glob("./data/instagram/*.txt")

for post in posts:
    f = open(post, "r").read()
    segmented_hastags = ""
    for hashtag in re.findall(hashtag_re, f):
        found_hashtag = "".join(hashtag)
        clean_hashtag = hashtag[1]
        segmented = " ".join(wordsegment.segment(clean_hashtag))
        tag = f"<exhashtag original='{clean_hashtag}'>{segmented}</exhashtag>"
        f = f.replace(found_hashtag, tag)
    print(f)









