{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d22760-c1c1-421b-bcdd-681639979ea1",
   "metadata": {},
   "source": [
    "# Working with digital textual data: a Python primer\n",
    "### aka *Things I wish someone had told me when I started using Python ~10 years ago*\n",
    "\n",
    "#### [https://github.com/mdic/python_primer](https://github.com/mdic/python_primer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b5e0f-e91c-4de8-9257-abe3cc0cdf38",
   "metadata": {},
   "source": [
    "# Disclaimer and scope\n",
    "The materials contained in this interactive Jupyter notebook are meant to provide a set of (some of the) main concepts and mechanisms underlying the use of Python for working with digital textual data. Rather than covering the \"basics\" of Python, it takes into account things that cover the full range of \"proficiency level\", from *basic* to *advanced*. As such, it is meant to be used as a cheat-sheet of \"things that need to be known\" when you will start experimenting with Python on your own.  \n",
    "\n",
    "This notebook is in no way meant to be an \"*introductory course to Python*\", nor is it meant to teach you how to write or use Python fluently.  \n",
    "Think of this notebook as a **heavily-opinionated** tour guide: it is the result of personal experience, and as such it is based on personal habits and needs. It's as if you wanted to visit a place you have never been to (e.g. India), and asked an Indian friend to provide you with a list of \"things to do and see in India\": while the list may be more or less complete, you will never know exactly what it means to experience any of the suggested things until you are there.  \n",
    "  \n",
    "When using and studying Python you will soon find out that a lot of the things written in this notebook are imprecise, and that a lot of the code exemplified could have been written in different ways. That is, you will soon find out that principle n.13 of the so-called [*Zen of Python*](https://en.wikipedia.org/wiki/Zen_of_Python):\n",
    "\n",
    "> There should be one-- and preferably only one --obvious way to do it.\n",
    "\n",
    "doesn't hold in real-life. Rather - just like natural languages - each person develops linguistic habits that contribute to a constantly varying - and subjective-first - ecosystem.  \n",
    "At last, let's be frank: using Python (just like any other programming language) is a *complex* endeavour. It's not difficult nor complicated, but **complex**; meaning that a lot of interconnected things (mostly from computer science, but from other fields as well) are involved. The only way effective way to learn a language (programming or natural one) is to experience it, and make a lot of mistakes!  \n",
    "  \n",
    "I hope this notebook will invite you to experiment first-hand with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2dc9b-b4ab-4cb4-bfc8-14305fee1476",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "You should run this notebook (i.e. this `.ipynb` file) from a local copy on your PC; this assumes that Python (or, even better, a Python virtual environment) is installed on your PC -  along with JupyterLab from which this notebook can be run - and that you have [downloaded the notebook files](https://github.com/mdic/python_primer/archive/refs/heads/main.zip). More details are available in section *Installing Python*.  \n",
    "You may then click on any cell (text or code) and modify it, adding notes, experiments, etc...  \n",
    "Nothing can go wrong, and you can also go back to the [original file](https://github.com/mdic/python_primer/blob/main/Python_primer.ipynb).  \n",
    "In addition, you may want to check out Di Cristofaro (2023) and the accompanying [online compendium `catlism`](https://catlism.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc256d-ef56-456e-a004-cca8575f90d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Why Python?\n",
    "\n",
    "The question might as well be \"why programming language(s)?\", but the answer remains pretty much the same.  \n",
    "\n",
    "> \"Some people think of corpus linguistics as the action that starts with the analysis of a corpus of texts: data is selected, collected, and processed, and only then does corpus linguistics begin. This is a narrow view and one that [should be broadened] by incorporating into corpus approaches those technical notions and procedures that define what corpus data is.\" (Di Cristofaro 2023:1)\n",
    "\n",
    "Approaching digital textual data should therefore be inclusive of everything that concerns data processing, or - to summarise - of *digital technicalities*\n",
    "\n",
    "> \"that is, those notions and mechanisms that â€“ while not classically associated with natural language â€“ are i) foundational of the digital environments in which language production and exchanges\n",
    "occur and ii) at the core of the techniques that are used to produce, collect, and process the focus of investigation, that is, digital textual data. (Di Cristofaro 2023:4)\n",
    "\n",
    "This does not however mean that we should become computer scientists nor expert coders, but rather\n",
    "\n",
    "> \"to acquire a degree of proficiency with the â€˜digital languageâ€™ sufficient enough to ensure that no disconnect is present among the digital data, the corpus itself, and the methods through which it is investigated and interpreted â€“ even when the person who collects the data is not the same one who conducts the analysis. (Di Cristofaro 2023:15)\n",
    "\n",
    "In fact we must not forget that\n",
    "> \"data processing is as relevant as data analysis; even more, it might be argued that the former is more crucial than the latter. Knowledge of how to use corpus tools and of the underlying theories is paramount to guarantee a scientifcally valid analysis and should never be overlooked or ignored. This can, however, be learnt or improved along the way during the analysis of the data once the corpus has already been created. Data from the web, on the contrary, does not usually permit this fexibility: ensuring that what a researcher needs for their analysis is correctly collected may be a one-time chance, not replicable in the future.\" (Di Cristofaro 2023:72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a58c28-543f-4e85-acb6-2761e4b82591",
   "metadata": {},
   "source": [
    "# Programming languages\n",
    "To simplify, the thousands of available programming languages can be categorised according to two major characteristics:\n",
    "\n",
    "a. Low- or high-level  \n",
    "b. General-purpose (GPL) or domain-specific (DSL) language  \n",
    "  \n",
    "**a.** A high-level programming language (such as Python) provides a \"strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language.\" ([source](https://en.wikipedia.org/wiki/High-level_programming_language)).  \n",
    "For this reason Python is considered to be among the most \"intuitive\" programming languages, since it uses English words as *keywords*: \"predefined, reserved words used in Python programming that have special meanings to the compiler. We cannot use a keyword as a variable name, function name, or any other identifier. They are used to define the syntax and structure of the Python language. All the keywords except `True`, `False` and `None` are in lowercase\" ([source](https://www.programiz.com/python-programming/keywords-identifier)).  Some examples are `for`, `if`, `else`, `with`, `as`, `and`, `or`; see [here](https://www.w3schools.com/python/python_ref_keywords.asp) or [here](https://www.geeksforgeeks.org/python-keywords/) for the full list.  \n",
    "This notebook will present a number of them through practical examples.\n",
    "  \n",
    "**b.** A general-purpose programming language (such as Python) can be used to develop a wide range of different applications (e.g. videogames, web apps, websites, editing images, processing textual data, etc...), while domain-specific ones are tailored to one (or a small number of) specific purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb9888-b3bc-434e-9d07-5448849be337",
   "metadata": {},
   "source": [
    "# Cooking with Python: a culinary analogy\n",
    "Let's pretend you have just bought a new apartment in Italy, and that it comes pre-furnished. In it you have a kitchen, a basic one; let's pretend it's a *basic Italian kitchen*. A kitchen where you will find all the tools and ingredients that are needed to prepare basic Italian dishes: an oven, a stove, some pans, some cutlery; pasta, salt, pepper, olive oil, tomatoes, onions, garlic.  \n",
    "You won't find Garam Masala in it, nor sushi rice, nor a wok. Just a basic Italian kitchen to get you started with Italian dishes.  \n",
    "The default Python installation is just like this kitchen: it has the basic tools and ingredients (called **modules** or **libraries**) and nothing more.  \n",
    "  \n",
    "Now, you have invited some friends over for an Indian dinner, and you are going to prepare a Tikka Masala curry. Your basic kitchen doesn't have a lot of the things needed - some of which are commonly available from any store in Italy, some others that can only be found in specialised stores. Chicken and yoghurt can easily be found in any supermarket; curry spices and coconut milk must be bought in Asian markets. \n",
    "Similarly in Python you may need to get some **modules from \"outside\" as they are not available in the basic installation**; this is commonly done through a module manager called [`pip`](https://pip.pypa.io/en/stable/).  \n",
    "\n",
    "What if one of your guests is coeliac? Well, you'd have to get some special ingredients, as well as using tools that must not come in contact with non-gluten-free ingredients. You would have to keep these ingredients and tools separate from the other ones, since mixing them may cause serious consequences: the former are conflicting with the latter.  \n",
    "The same may happen with Python modules: you may be using a module that requires specific versions of other modules, which in turn may be incompatible with another module. Luckily with Python you can create a **virtual environment** (often called `venv`), a self-contained and isolated \"box\" into which you can install modules and keep them separated from modules installed inside of a second virtual environment.  \n",
    "\n",
    "So our next step is to install Python through `conda`, a package manager which simplifies the installation of modules as well as the creation of virtual environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678a4c4-da90-4e77-b9cd-88234dd29124",
   "metadata": {},
   "source": [
    "# Installing Python\n",
    "\n",
    "[Miniconda](https://docs.anaconda.com/free/miniconda/index.html) or alternatively [Miniforge](https://conda-forge.org/miniforge/) (for the latter, select the installer for **Miniforge3**, the last in the table with the download links). Setup guides for Miniconda are available for [Windows](https://katiekodes.com/setup-python-windows-miniconda/) and [macOS](https://medium.com/@sophieowen_40339/how-to-install-conda-and-create-virtual-environments-on-mac-m1-a3a15093820b).  \n",
    "Miniconda/Miniforge will install Python in your system, and automatically create a virtual environment called `base` which replaces (not delete, but \"take the place of\") your existing Python installed on your system (if you had already installed Python!).  \n",
    "Once installed, opening a command prompt (Windows) or terminal (macOS) will show something like the following:\n",
    "\n",
    "> `(base) catlism@debian:~$`\n",
    "\n",
    "where your username, name of the PC, and name of the folder you are currently in is preceded by the label `(base)`. This is the way through which `conda` lets you know that you are currently using its basic (default) virtual environment.  \n",
    "From here we can create a new environment called `test` by writing the following command followed by `Enter`:\n",
    "\n",
    "> `conda create --name test`\n",
    "\n",
    "which we can then activate using the command:\n",
    "\n",
    "> `conda activate test`\n",
    "\n",
    "This will be reflected in the terminal, where something like the line below will replace the previous `(base)` version:\n",
    "\n",
    "> `(test) catlism@debian:-$`\n",
    "\n",
    "If you want to deactivate the `test` environment and switch back to the `base` one, use the following command:\n",
    "\n",
    "> `conda deactivate`\n",
    "\n",
    "Further details may be found in [this page](https://catlism.github.io/setup_env/conda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d798a-134c-49e5-a6d4-733cfa5aa5fe",
   "metadata": {},
   "source": [
    "## Installing JupyterLab to run this notebook\n",
    "\n",
    "Now, let's create a `venv` for this notebook, along with the required packages needed to run it.\n",
    "\n",
    "> `conda create --name primer`\n",
    "\n",
    "We now activate the just created `primer` virtual env\n",
    "\n",
    "> `conda activate primer`\n",
    "\n",
    "And at last we install the required modules\n",
    "\n",
    "> `pip install jupyterlab python-lsp-server[all]`\n",
    "\n",
    "We can now start JupyterLab\n",
    "\n",
    "> `jupyter lab`\n",
    "\n",
    "and then, once inside JupyterLab, load this notebook by double-clicking on the file *Python_primer.ipynb*.  \n",
    "\n",
    "\n",
    "## Notes\n",
    "1. Remember: Jupyter Lab will use as working folder the folder in which the command `jupyter lab` is run)\n",
    "2. Windows and macOS use different syntax to identify paths (i.e. the locations of folders and files); the path to the Instagram data for this notebook files is seen by Windows as `data\\instagram\\`, while macOS reads it as `data/instagram/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b378e-1b91-4792-a108-f0276b606405",
   "metadata": {},
   "source": [
    "# The two golden rules\n",
    "In order to begin approaching Python code, two rules need to be known:\n",
    "\n",
    "1. Strings of text preceded by a `#` symbol, or enclosed in three pairs of single or double quotes (`'''` or `\"\"\"`) are **comments meant for humans and are not read by Python**. Everything else is interpreted by Python as code.\n",
    "2. Graphical indentation is meaningful in Python, and defines the hierarchy of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152e127-90ab-4fa6-aff6-2c0e1d2dfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a single-line comment\n",
    "\n",
    "\"\"\"\n",
    "This is a\n",
    "multiline\n",
    "comment\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d4c45-f895-47bb-8974-226d80939c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the following code has a hierarchy, whereby `print(c)` is a child of `for c in \"example\"`\n",
    "for c in \"example\":\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf3146-cf28-4db1-a571-0e5b80a8934d",
   "metadata": {},
   "source": [
    "# Types of data\n",
    "Python is able to read different types of data, and do different things with each one of them. This is similar to what humans can and cannot do: we can add/divide/multiply/subtract numbers, but not letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17047ddf-601f-4253-9823-1ebe82931beb",
   "metadata": {},
   "source": [
    "## Numbers\n",
    "Python (just like the majority of programming languages) distinguishes [numbers](https://www.w3schools.com/python/python_numbers.asp) by grouping them into two sub-categories: `integers` and `floats` (a third type exists, but we'll ignore it as we're not going to need it).  \n",
    "In the code below, we assign four different numbers to four different variables (`w`, `x`, `y`, `z`), and ask Python to print the `type` of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3b835-b78e-40d7-93bc-f9bdb8b93640",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1\n",
    "x = -3255522\n",
    "y = 1.09834\n",
    "z = -20.976362\n",
    "\n",
    "print(type(w))\n",
    "print(type(x))\n",
    "print(type(y))\n",
    "print(type(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55dc5d-dcc5-49c0-b4f1-938ffecfc6f5",
   "metadata": {},
   "source": [
    "## Text strings\n",
    "A string of text is defined by enclosing it in single or double quotes.  \n",
    "In the example below, we assign a string to the variable called `text`; then we ask Python to operate on the string. We want Python to take each minimal unit  - which we call `c` - of the object stored in `text` and print it, one after the other until the object (i.e. the string) is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5765f9-452e-484d-8c48-f45cd09bc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "\n",
    "for c in text:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b898d-0700-454a-9977-89c2171cafde",
   "metadata": {},
   "source": [
    "By default Python sees as minimal unit of a string what we humans call a character.  \n",
    "We can change this behaviour by applying one or more **methods**.  \n",
    "A **method** is a special word that apply a function (the parentheses `()` following the special word indicate it is a function) to the variable that precedes them (the dot `.` indicates that the function is applied to what is on the left of it).  \n",
    "For example the [`.split()`](https://www.w3schools.com/python/ref_string_split.asp) method splits a string of text whenever it finds a whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b416f-1612-4719-baa2-d534cdde2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in text.split():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3b1a5-86d0-4b78-a768-4e5a1e6b56d3",
   "metadata": {},
   "source": [
    "## Lists\n",
    "\n",
    "Lists are used to store multiple items (e.g. strings or numbers) in a single variable, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49207abd-7813-4df6-9000-755ef715584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = [\"chicken\", \"curry spices\", \"yoghurt\", \"coconut milk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a8e49-5d02-4172-979c-94f5bc66d73c",
   "metadata": {},
   "source": [
    "The four elements are seen by Python as the minimal units of the object `ingredients`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0b4a2-d39b-4a25-a891-99a4f23e0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ingredients:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8e568-71e0-44a9-b953-ce266107ed32",
   "metadata": {},
   "source": [
    "List items are **ordered**, **changeable**, **allow duplicate values**, and are **indexed** - i.e. the first item has index `[0]`, the second item has index `[1]` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee567b62-e92b-4374-ad72-4d68a9eb7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists are ordered and indexed\n",
    "\n",
    "print(ingredients[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594a089-af5f-4d65-a076-3aba4bad8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists are changeable\n",
    "ingredients[2] = \"vegan yoghurt\"\n",
    "print(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd1ab2-09f6-439b-8378-2eeeda47dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists may contain duplicate values\n",
    "ingredients.append(\"chicken\")\n",
    "# we need A LOT of chicken!\n",
    "print(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ee90d-26fe-41e5-8740-32072992ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many items are in a list?\n",
    "print(len(ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356254f3-f74f-4089-9d0b-fc4eac7abf03",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "Dictionaries are used to store data values in `key:value` pairs.\n",
    "\n",
    "A dictionary is a collection which is **ordered**, **changeable** and **do not allow duplicates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae502692-274f-41e9-81ab-120997492457",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_list = {\n",
    "    \"chicken\": \"1 whole\",\n",
    "    \"curry spices\": \"150gr\",\n",
    "    \"yoghurt\": \"200gr\",\n",
    "    \"coconut milk\": \"400ml\",\n",
    "}\n",
    "\n",
    "print(shopping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb146ed-91b9-4102-8585-fb26dc6c1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries are ordered\n",
    "print(shopping_list[\"chicken\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62692b9-6cab-49af-8fc0-a081dbd4bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries are changeable\n",
    "shopping_list[\"yoghurt\"] = \"400gr\"\n",
    "print(shopping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309c02a-40b7-4f14-9b78-7186f4e00a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries do not allow duplicates\n",
    "shopping_list = {\n",
    "    \"chicken\": \"1 whole\",\n",
    "    \"curry spices\": \"150gr\",\n",
    "    \"yoghurt\": \"200gr\",\n",
    "    \"coconut milk\": \"400ml\",\n",
    "    \"coconut milk\": \"300ml\",\n",
    "}\n",
    "\n",
    "print(shopping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2df84-e4df-46c0-9c58-3a1350e7777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many items are in a dictionary?\n",
    "print(len(shopping_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920c096-3c88-4f43-adc0-926a61c831ea",
   "metadata": {},
   "source": [
    "## Other types\n",
    "Other types of data exist, but we're not going to cover them here (nor usem them!). You may read more details about them [here](https://python101.pythonlibrary.org/chapter3_lists_dicts.html) and [here](https://www.w3schools.com/python/python_datatypes.asp). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab301020-2e92-4656-a044-6803b5c33d07",
   "metadata": {},
   "source": [
    "# Regular expressions\n",
    "\n",
    "> Regular expressions (also known as regexes or regex patterns) are strings of text interpreted by a software or a programming language as rules for matching one or more patterns inside a set of strings [...] Regexes are extremely flexible and can be used to match any type of pattern, from simple words to email addresses, to more complex constructions; this flexibility comes at\n",
    "the expense of their readability, as they can become extremely complex to interpret or to build. The easiest way to approach them and to learn their syntax is by using a tool such as [RegExr](https://regexr.com/) (Skinner 2022), an open source application for creating, testing, and learning regular expressions (Di Cristofaro 2023:130-132)\n",
    "\n",
    "Many of the code examples below make use of regular expressions through the built-in library `re`, and we're going to see through such examples some use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb7626-caa7-49ee-80aa-2084270c80ad",
   "metadata": {},
   "source": [
    "# Reading files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a54927-ff64-4328-957f-aaf63c906ef8",
   "metadata": {},
   "source": [
    "## Reading one single file\n",
    "Reading a file entails two operations: first we `open` it, then we `read` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de4836-d9d3-497d-9605-28628710a107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/instagram/2022-01-02_14-00-14_UTC.txt\", \"r\") as text:\n",
    "    print(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a5ea8-d853-402a-84ce-42e0c435336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/instagram/2022-01-02_14-00-14_UTC.txt\", \"r\") as text:\n",
    "    print(text.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10c6b7-93aa-4bf3-882b-b9fe4f888b90",
   "metadata": {},
   "source": [
    "## Reading multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f46536-46bb-4715-b251-ac4f605d9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for file in files:\n",
    "    text = open(file, \"r\").read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f09a90-58a7-42eb-ab14-922f6fcd9e0f",
   "metadata": {},
   "source": [
    "# Writing to file(s)\n",
    "The are different `modes` to write data to a file; the most important ones to work with digital textual data are `w` and `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21733c95-35f8-4f2f-bc6a-a741d006ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = [\"chicken\", \"curry spices\", \"yoghurt\", \"coconut milk\"]\n",
    "more_ingredients = [\"rice\",\"ghee\",\"salt\",\"bay leaves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade7624-ff6a-45f4-8b3a-dd9d39dc88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"list_of_ingredients.txt\", \"w\")\n",
    "for i in ingredients:\n",
    "    out.write(f\"{i}\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd073cc1-9f83-49d1-bb44-6d6cc8bdeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"list_of_ingredients.txt\", \"w\")\n",
    "for m in more_ingredients:\n",
    "    out.write(f\"{m}\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b95a6-bb86-42b0-821e-fc5525bacde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"list_of_ingredients.txt\", \"a\")\n",
    "for i in ingredients:\n",
    "    out.write(f\"{i}\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259469fb-ba91-497a-b4b8-679a0c9fc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"list_of_ingredients.txt\", \"a\") as out:\n",
    "    for i in ingredients:\n",
    "        out.write(f\"{i}\\n\")\n",
    "    for m in more_ingredients:\n",
    "        out.write(f\"{m}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed3b8b-0806-405d-97f0-7d997d599951",
   "metadata": {},
   "source": [
    "# Tabular data: `csv`\n",
    "\n",
    "Exemplified below is an advanced usage of `pandas`, a module made for working with a very powerful version of spreadsheets called `dataframes`. The module is one of the most important tools for data science, and has a lot of features used to process, transform, and analyse data stored in tabular format.  \n",
    "An easier - and less powerful option - to work with tabular data is the module `csv` (included in any default Python installation, but requires importing through the line `import csv` at the beginning of a script).  \n",
    "Whenever we work with tabular data, it's always a good choice to save it in `csv` since other formats (e.g. Excel `xlsx`) are not *open* and may cause data to be incomprehensible to Python.  \n",
    "So, when working with Excel/Google Sheets/etc... select `csv` from the \"Save as\" option (see Di Cristofaro 2023:104-116 for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99ed3c-a8dc-410d-b0f6-1c2cd074d95a",
   "metadata": {},
   "source": [
    "Code below exemplifies a common operation when working with digital textual data: we have a `csv` file where we have saved metadata (additional, often manually-entered) details for the Instagram files in the `instagram` folder. At some point we will likely need to merge the source data with our metadata details, and `pandas` can help us automate this operation (thus reducing the risk of errors) by using the metadata `csv` file as a **lookup table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582d5d7-13ca-4e95-ade8-410623207a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(\"./data/metadata.csv\", sep=\"\\t\")\n",
    "metadata = metadata.set_index(\"filename\")\n",
    "metadata = metadata.groupby(metadata.index).first()\n",
    "\n",
    "print(metadata.loc[\"2022-12-12_22-00-37_UTC.txt\", \"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cf6fa-380b-41fc-aa6b-e6a580a5fc9b",
   "metadata": {},
   "source": [
    "# Marked-up data: `xml`\n",
    "Arguably the single most important format for working with digital textual data, `xml` is a markup language that allows us to include additional information alongside text contents (see Di Cristofaro 2023:105-111 for more details and for the basic rules of `xml`).  \n",
    "At their core, `xml` files are text file were things enclosed in tags (i.e. between an opening `<` and a closing `>`) have a special meaning, and are interpreted by computers as special objects.  \n",
    "Tags create a structure, which **parsers** (such as Python `lxml` or `beautifulsoup`) can read and understand, allowing them to navigate inside the data using said structure. As such, we should never treat `xml` as \"text files only\", meaning that using **regular expressions** to create/edit them is (almost) always a bad idea!  \n",
    "The code below exemplifies some basic operations using a dataset of webpages collected in `xml` through [`trafilatura`](https://trafilatura.readthedocs.io/en/latest/) (see also Di Cristofaro 2023:156-160; [catlism](https://catlism.github.io/data_collection/general_purpose/trafilatura.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be49a7-f4d7-44ab-b358-262109b867e2",
   "metadata": {},
   "source": [
    "### Parsing existing metadata\n",
    "The first example shows how to extract the value of the attribute `author` in each `xml` file. But first we need to install beautifulsoup and lxml through\n",
    "\n",
    "> `pip install beautifulsoup4 lxml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f32f1-478f-4a2b-bb14-cd38de26fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "\n",
    "data = glob(\"./data/xml/*.xml\")\n",
    "\n",
    "for d in data:\n",
    "    f = open(d, encoding=\"utf-8\")\n",
    "    soup = BeautifulSoup(f, \"lxml\")\n",
    "    author = soup.find(\"doc\")[\"author\"]\n",
    "    print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacddd5-095b-48bb-b5c7-7b5d22749f77",
   "metadata": {},
   "source": [
    "### Changing the name of a tag in an xml file\n",
    "The first example involves parsing existing `xml` files, and changing the main (root) tag `doc` into `text`, so that they are compatible with all the corpus tools that accept `xml` files as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc03d-be0f-4aea-a486-eaef5b8558b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to install beautifulsoup and lxml through\n",
    "# pip install beautifulsoup4 lxml\n",
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "\n",
    "data = glob(\"./data/xml/*.xml\")\n",
    "\n",
    "for d in data:\n",
    "    f = open(d, encoding=\"utf-8\")\n",
    "    filename = file.replace(\".xml\", \"\")\n",
    "    soup = BeautifulSoup(f, \"lxml\")\n",
    "    doc_tag = soup.find(\"doc\")\n",
    "    doc_tag.name = \"text\"\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da764709-5d1a-48a9-a42e-f13bc6b8e13a",
   "metadata": {},
   "source": [
    "### Creating xml files\n",
    "In this last example we switch back to Instagram data, and create an `xml` file for each post, adding the metadata for each post from the previously seen `csv` file. This time we use `lxml` instead of `beautifulsoup` for a change - `lxml` can read (parse) and write `xml` files; `beautifulsoup` can read (parse) and write `xml` and `html` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541a710-2d8a-49f8-9843-09920a7db941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(\"./data/metadata.csv\", sep=\"\\t\")\n",
    "metadata = metadata.set_index(\"filename\")\n",
    "metadata = metadata.groupby(metadata.index).first()\n",
    "\n",
    "data = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for d in data:\n",
    "    filename = re.sub(\".*?instagram\\/\", \"\", d)\n",
    "    f = open(d, \"r\").read()\n",
    "    root_tag = etree.Element(\"text\")\n",
    "    root_tag.attrib[\"type\"] = metadata.loc[f\"{filename}\", \"type\"]\n",
    "    root_tag.text = f\n",
    "    tree = etree.ElementTree(root_tag)\n",
    "    tree.write(\n",
    "        f\"./data/instagram/{filename}.xml\",\n",
    "        pretty_print=True,\n",
    "        xml_declaration=True,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c7bed-5d3e-471f-988a-7c448319e525",
   "metadata": {},
   "source": [
    "# Language recognition\n",
    "As with our Instagram data, it may be the case that our dataset contains multiple languages - an issue if we are going to use e.g. corpus tools to analyse it!  \n",
    "Luckily there are many libraries that allow Python to identify which language is used in a string/text; one such library is [`lingua-py`](https://github.com/pemistahl/lingua-py).  \n",
    "We install it with the command  \n",
    "\n",
    "> `pip install lingua-language-detector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d6599-0931-4ae4-98e4-c5c35501b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "# Setup some variables and parameters to be later used\n",
    "languages = [Language.ENGLISH, Language.FRENCH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30779ac-69dd-4777-bf47-e098c9501b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for post in posts:\n",
    "    f = open(post, \"r\").readlines()\n",
    "    for line in f:\n",
    "        lang = detector.detect_language_of(line)\n",
    "        print(f\"TEXT::{line}\\nLANG::{lang}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97e6d4-4f75-4e1e-b986-a1d48538203a",
   "metadata": {},
   "source": [
    "Alternatively, we can use `lingua-py` to separate text in English from text in French by creating two separate files, named after the original post filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9cf54-aaf6-4575-886a-09b0ab8e0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for post in posts:\n",
    "    f = open(post, \"r\").readlines()\n",
    "    post_lines = {}\n",
    "    recognised_lang = \"\"\n",
    "    post_name = re.sub(\".*?instagram\\/\", \"\", post).replace(\"/\", \"\").replace(\".txt\", \"\")\n",
    "    for line in f:\n",
    "        lang = detector.detect_language_of(line)\n",
    "        if lang is None:\n",
    "            lang = recognised_lang\n",
    "        else:\n",
    "            post_lines[f\"{line}\"] = lang\n",
    "    lines_en = [k for k,v in post_lines.items() if str(v) == \"Language.ENGLISH\"]\n",
    "    lines_fr = [k for k,v in post_lines.items() if str(v) == \"Language.FRENCH\"]\n",
    "    post_en = open(f\"{post_name}_EN.txt\", \"a\").write(\"\\n\".join(lines_en))\n",
    "    post_fr = open(f\"{post_name}_FR.txt\", \"a\").write(lines_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664896eb-c8b2-4941-a8d3-d0d5d3e2de0c",
   "metadata": {},
   "source": [
    "# Emoji transliteration\n",
    "Emojis pose a number of issues when working with corpus tools (AntConc, SketchEngine, LancsBox, LancsBox X, WordSmith, etc...) since they are often not fully supported (yet). The reason is extremely complex, but the consequences are straightforward: corpus tools don't \"see\" emojis as we humans do, and - more often than not - miscount and misrepresent them. This leads in turn to a skewed corpus and skewed results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0214899-624e-49a7-af13-51255901fe13",
   "metadata": {},
   "source": [
    "## Emoji consequences\n",
    "For example, the following \"sentence\" is interpreted by us as composed of 3 different emojis, while corpus tools may interpret them in various ways, including seeing 6 (or more) emojis.\n",
    "\n",
    "> ðŸ‘¯â€â™‚ï¸ ðŸ‘©ðŸ¿â€ðŸ¦° ðŸ¥·ðŸ¼\n",
    "\n",
    "The three emojis above all have an official description (called [CLDR](https://unicode.org/emoji/charts/full-emoji-list.html)) provided by the Unicode consortium. These are:  \n",
    "\n",
    "- [Men with Bunny Ears](https://emojipedia.org/men-with-bunny-ears)\n",
    "- [Woman: Dark Skin Tone, Red Hair](https://emojipedia.org/woman-dark-skin-tone-red-hair)\n",
    "- [Ninja: Medium-Light Skin Tone](https://emojipedia.org/ninja-medium-light-skin-tone)\n",
    "\n",
    "The peculiarity of these 3 emojis (and of a number of other ones) is that they are created by combining together two or more emojis:  \n",
    "\n",
    "> ðŸ‘¯ People with Bunny Ears + â™‚ï¸ Male Sign  \n",
    "> ðŸ‘© Woman + ðŸ¿ Dark Skin Tone + ðŸ¦° Red Hair  \n",
    "> ðŸ¥· Ninja + ðŸ¼ Medium-Light Skin Tone  \n",
    "\n",
    "To avoid issues - and have a corpus as faithful as possible to the original data - we may *transliterate* emojis into their CLDR, thus turning each emoji into a \"bundle of words\" that contain their official descriptions; e.g.\n",
    "\n",
    "> ðŸ‘¯â€â™‚ï¸ -> {men_with_bunny_ears}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5562505-1540-4c69-8bd2-f3ed343f4778",
   "metadata": {},
   "source": [
    "The syntax used for the description is applied so that the corpus tool doesn't interpret each word of the description as a token, but rather treats the `{men_with_bunny_ears}` as a single (unknown) token.  \n",
    "Emoji transliteration can be achieved through the Python module [`emoji`](https://github.com/carpedm20/emoji) (see also [here](https://catlism.github.io/data_processing/emoticons_emojis.html)),which we now install with the command\n",
    "\n",
    "> `pip install emoji`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfa811-676c-4a38-8daa-7b3168b75541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Define a custom function to transliterate emojis and add curly brackets as delimites for the CLDR\n",
    "\n",
    "def demoji(chars, data_dict):\n",
    "    trans = emoji.demojize(chars, delimiters=(\"{\", \"}\"))\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93a846-a345-437c-a2cf-4b7fdd4fe2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for post in posts:\n",
    "    f = open(post, \"r\").readlines()\n",
    "    for line in f:\n",
    "        line = emoji.replace_emoji(line, replace=lambda chars, data_dict: demoji(chars,data_dict))\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c65ec-40b0-4063-ad98-cd8a95ffc2b0",
   "metadata": {},
   "source": [
    "# Hashtag segmentation\n",
    "\n",
    "Oftentimes hashtags are formed by two or more words merged together, making the actual linguistic contents incomprehensible to language analysis tools.  \n",
    "The following script is an adaptation of [s5.17](https://github.com/catlism/catlism_scripts/raw/main/s5.17_wordsegment_hashtags.py) (Di Cristofaro 2023), and uses the module [`wordsegment`](https://github.com/grantjenks/python-wordsegment) to identify when two or more words are merged and subsequently split them. More info can be found [here](https://catlism.github.io/data_processing/hashtags.html); we now install the library with the command\n",
    "\n",
    "> `pip install wordsegment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ab84d-85c8-4fda-8727-49c393620431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import wordsegment\n",
    "\n",
    "wordsegment.load()\n",
    "hashtag_re = re.compile(\"(?:^|\\s)([ï¼ƒ#]{1})(\\w+)\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd0031-1c1b-40c6-88d2-538dd2db7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = glob(\"./data/instagram/*.txt\")\n",
    "\n",
    "for post in posts:\n",
    "    f = open(post, \"r\").read()\n",
    "    segmented_hastags = \"\"\n",
    "    for hashtag in re.findall(hashtag_re, f):\n",
    "        found_hashtag = \"\".join(hashtag)\n",
    "        clean_hashtag = hashtag[1]\n",
    "        segmented = \" \".join(wordsegment.segment(clean_hashtag))\n",
    "        tag = f\"<exhashtag original='{clean_hashtag}'>{segmented}</exhashtag>\"\n",
    "        f = f.replace(found_hashtag, tag)\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288d8f0-9293-4bbb-ab23-b45e74c858ab",
   "metadata": {},
   "source": [
    "# Where to next?\n",
    "What follows is a list of \"topics\" and suggested \"materials\" (e.g. names of modules, technical terms used to refer to a topic, references, guides, etc...) you may want to investigate and consider while continuing your journey into Python.  \n",
    "You may also want to consult some Python guides, such as:\n",
    "- [Python 101](https://python101.pythonlibrary.org/index.html)\n",
    "- [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html)\n",
    "- [Tutorials on W3Schools](https://www.w3schools.com/python/default.asp)\n",
    "\n",
    "|topic|terms|\n",
    "|---|---|\n",
    "|markdown|[markdownguide](https://www.markdownguide.org/); also this entire notebook (at least the textual parts) are written in markdown, so you may want to double click on a cell and see how it was written|\n",
    "|manipulating text strings|f-strings (i.e. the syntax `f\"\"`)|\n",
    "|character encodings/UTF-8/Unicode|[The Unicode Cookbook for Linguists: Managing writing systems using orthography profiles](https://zenodo.org/records/1300528); [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/); [What Every Programmer Absolutely, Positively Needs to Know About Encodings and Character Sets to Work With Text](https://kunststube.net/encoding/)|\n",
    "|tabular data (reading/parsing/editing/creating)|[pandas](https://www.w3schools.com/python/pandas/default.asp)|\n",
    "|HTML data (reading/parsing)|[beautifulsoup](https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/)|\n",
    "|XML data (reading/parsing/editing/creating)|[lxml](https://lxml.de/tutorial.html); [beautifulsoup](https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/)|\n",
    "|deleting/renaming/creating local files/folders|[os](https://www.w3schools.com/python/module_os.asp)|\n",
    "|digital textual data formats|along with `csv` and `xml`, also check out `json` (Di Cristofaro 2023:104-116)|\n",
    "|linguistic annotation/tagging|[PyMUSAS](https://ucrel.github.io/pymusas/), which powers WMatrix, and is powered by [Spacy](https://spacy.io/) - THE most important NLP Python library|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
